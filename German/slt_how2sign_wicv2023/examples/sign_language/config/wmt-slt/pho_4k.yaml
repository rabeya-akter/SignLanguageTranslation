# @package _global_

hydra:
  run:
    dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

common:
  seed: 48151623
  fp16: False
  wandb_project: ${env:WANDB_PROJECT}

distributed_training:
  heartbeat_timeout: 600

checkpoint:
  save_dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/ckpts/
  keep_last_epochs: 1
  save_interval: 5
  best_checkpoint_metric: sacrebleu
  maximize_best_checkpoint_metric: True
  keep_best_checkpoints: 10
  patience: 10

dataset:
  train_subset: phoenix.mediapipe.train
  valid_subset: phoenix.mediapipe.dev
  validate_interval: 5
  num_workers: 4
  max_tokens: 16_000                          # 640s @ 25fps
  skip_invalid_size_inputs_valid_test: False

task:
  _name: sign_to_text
  data: ${env:DATA_DIR}
  max_source_positions: 1024                  # ~41s @ 25fps
  max_target_positions: 1024
  normalization: body
  eval_gen_config:
    beam: 5
  eval_bleu: True
  eval_bleu_config:
    sacrebleu_tokenizer: 13a
    sacrebleu_lowercase: False
    sacrebleu_char_level: False
  eval_print_samples: True

model:
  _name: sign2text_transformer
  encoder_embed_dim: 256
  encoder_ffn_embed_dim: 1024
  encoder_attention_heads: 4
  encoder_layers: 3
  decoder_embed_dim: 256
  decoder_ffn_embed_dim: 1024
  decoder_attention_heads: 4
  decoder_layers: 3
  decoder_output_dim: 256
  layernorm_embedding: True
  share_decoder_input_output_embed: True
  dropout: 0.2
  attention_dropout: 0.2
  activation_dropout: 0.2
  activation_fn: relu

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.1

optimizer:
  _name: adam
  adam_betas: (0.9, 0.98)
  adam_eps: 1e-08

optimization:
  lr: [5e-04]
  max_update: 40_000
  update_freq: [1]
  clip_norm: 1

lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 2000

bpe:
  sentencepiece_model: ${env:DATA_DIR}/wmtslt22.focusnewsandsrfandphoenix.unigram4000.model
